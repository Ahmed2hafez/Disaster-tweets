{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85c9050e-2e22-4f8d-bf75-8fba113c7d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c19f9d-bfa4-4c07-886a-e23d9b408638",
   "metadata": {},
   "source": [
    "# 1- Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9b9a6919-f132-4cd7-ac40-9b8784a2e0d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath=\"../project/data/\"\n",
    "data = pd.read_csv(f\"{filepath}train.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1dfccd-9386-41ef-860b-5ff8bda049fc",
   "metadata": {},
   "source": [
    "# 2- Dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7159f0c-0ed8-4f54-9795-7b2d087efcc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set information:\n",
      "---------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data has 7613 rows and, 5 column\n",
      "\n",
      "Proportion of samples with label 1 (ie A real disaster tweet): 42.97 %\n",
      "Proportion of samples with label 0 (ie Not a real disaster tweet): 57.03 %\n",
      "\n",
      "Number of Missing Tweets = 0\n",
      "Number of Missing Targets = 0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining set information:\")\n",
    "print(\"---------\")\n",
    "sns.countplot(x=\"target\", data=data)\n",
    "plt.show()\n",
    "\n",
    "print(f'Input data has {len(data)} rows and, {len(data.columns)} column\\n')\n",
    "\n",
    "print(\"Proportion of samples with label 1 (ie A real disaster tweet):\", np.round(data[\"target\"].sum()/len(data)*100,2), \"%\")\n",
    "print(\"Proportion of samples with label 0 (ie Not a real disaster tweet):\", np.round((1-data[\"target\"].sum()/len(data))*100,2),\"%\\n\")\n",
    "\n",
    "\n",
    "print(f'Number of Missing Tweets = {data[\"text\"].isnull().sum()}')\n",
    "print(f'Number of Missing Targets = {data[\"target\"].isnull().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86815d26-2c1d-41d4-9006-91317d7e1b99",
   "metadata": {},
   "source": [
    "# 3- Dataset Cleaning\n",
    "As the text in these samples are tweets, there may be twitter handles or hashtags that are nonsense, but some handles for example could be for an emergency service account which can help us in our prediction. There also may be links in a tweet, which will not be as helpful and we can remove. \n",
    "\n",
    "The function will \n",
    "* remove the @ and # of handles and hashtags\n",
    "* set text to lowercase\n",
    "* remove html characters\n",
    "* remove Emails ex. sebaiy@gmail.com\n",
    "* convert apostrophe contractions to their full counterparts ex. i'm ==> i am\n",
    "* remove hyperlinks\n",
    "* remove stopwords\n",
    "* remove punctuation \n",
    "* remove weird quirks from tweets such as \"û_\" and \"vÌ_deo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0bd1616d-9030-4afc-aa15-64233be6ec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import ImageColorGenerator\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "28bb6d28-3f79-465f-9235-004ea9b09a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "wnl = WordNetLemmatizer()  \n",
    "set(string.punctuation)\n",
    "stop_words.add('u')\n",
    "\n",
    "Apos_dict={\"'s\":\" is\",\"won't\":\"will not\", \"can't\":\"cannot\",\"n't\":\" not\",\"'m\":\" am\",\"'ll\":\" will\",\n",
    "           \"'d\":\" would\",\"'ve\":\" have\",\"'re\":\" are\"}\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    text=text.lower()\n",
    "    \n",
    "    text=html.unescape(text) # remove html tag words\n",
    "    \n",
    "    for key,value in Apos_dict.items(): # remove contractions\n",
    "        if key in text:\n",
    "            text = text.replace(key,value)\n",
    "\n",
    "    text=re.sub(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{1,4}\\b\", \"\", text) #emails like (sebaiy.mohamed@gmail.com)\n",
    "    \n",
    "    text=re.sub(r\"[À-ÿ]+\", \"\", text) #remove sympols like ([À, Ç, Ë,.., ÿ)\n",
    "    \n",
    "    text=re.sub(r'https?:\\/\\/.\\S+', \"\",text)    # remove links\n",
    "    \n",
    "    text=re.sub(r'\\S*û_*\\S*|@\\w*|#\\w*', \"\",text) # remove u_, handles and just the hashtag froms hashtags\n",
    "    \n",
    "    text=re.sub(r'[0-9]', \"\", text) #remove numbers\n",
    "\n",
    "    text=re.sub(r'[^\\w\\s]', \"\",text)# remove any character not in the set [^] ie punctuation, can add # | @ here\n",
    "\n",
    "    no_stopwords = [i for i in  WhitespaceTokenizer().tokenize(text) if i not in stop_words]\n",
    "    \n",
    "    n_lemma = [wnl.lemmatize(i, pos= 'n') for i in  no_stopwords] # nouns Lemmatizations\n",
    "    v_lemma = [wnl.lemmatize(i, pos= 'v') for i in  n_lemma] # verb Lemmatizations\n",
    "    a_lemma = [wnl.lemmatize(i, pos= 'a') for i in  v_lemma] # adjectives Lemmatizations\n",
    "    r_lemma = [wnl.lemmatize(i, pos= 'r') for i in  a_lemma] # adverbs Lemmatizations\n",
    "    s_lemma = [wnl.lemmatize(i, pos= 's') for i in  r_lemma] # satellite adjectives Lemmatization\n",
    "    \n",
    "    # remove punctuation\n",
    "    no_punct = []\n",
    "    \n",
    "    for word in s_lemma:\n",
    "        s = \"\"\n",
    "        for ch in word:\n",
    "            if ch not in set(string.punctuation):\n",
    "                s = f'{s}{ch}'\n",
    "        no_punct.append(s)\n",
    "        \n",
    "    text = \" \".join(no_punct)       \n",
    "    return text\n",
    "\n",
    "#print(f\"An example\\nThe uncleaned tweet: {data['text'].loc[71]}\")\n",
    "#print(f\"The cleaned tweet: {clean_text(data['text'].loc[71])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb61c41-2729-4c2d-8e8e-4f38695e2235",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aea8c528-017f-4383-b998-d90bedf82104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>deed reason may allah forgive u</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>resident ask ishelter place notify officer eva...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>people receive evacuation order california</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>get send photo ruby smoke pour school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0                    deed reason may allah forgive u       1\n",
       "1              forest fire near la ronge sask canada       1\n",
       "2  resident ask ishelter place notify officer eva...       1\n",
       "3         people receive evacuation order california       1\n",
       "4              get send photo ruby smoke pour school       1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data = pd.DataFrame()\n",
    "\n",
    "cleaned_data[\"text\"] = data[\"text\"].apply(clean_text)\n",
    "\n",
    "cleaned_data[\"target\"] = data[\"target\"]\n",
    "cleaned_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e2db32c-d687-4f53-a2f0-82677b8c5ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0098d4fc-a2c8-4e7e-861b-c06671e4db40",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_data, x_test_data, y_train, y_test = model_selection.train_test_split(cleaned_data[\"text\"], cleaned_data[\"target\"], test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cbccbfe4-4bbd-47cd-9440-adb382f1d397",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorisers={\"tf_idf\": TfidfVectorizer(stop_words='english',\n",
    "                                                          analyzer=\"word\",\n",
    "                                                          ngram_range=(1,2),\n",
    "                                                          max_features=None,\n",
    "                                                          min_df=0.0004)}\n",
    "vec = vectorisers[\"tf_idf\"]\n",
    "vec_train = np.array(cleaned_data[\"text\"])\n",
    "vec = vec.fit(np.array(vec_train))\n",
    "\n",
    "x_train = vec.transform(np.array(x_train_data))\n",
    "x_test = vec.transform(np.array(x_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410fc87b-851c-4acc-926f-5890f7e357c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "952d26d0-7439-4642-99a4-47ef2c608c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: update california hwy close direction due lake county fire\n",
      "*******************************\n",
      "Vectorised tweet:\n",
      "  (0, 3503)\t0.23323725332601125\n",
      "  (0, 3500)\t0.17889076068258508\n",
      "  (0, 3334)\t0.23323725332601125\n",
      "  (0, 3333)\t0.23323725332601125\n",
      "  (0, 2172)\t0.23323725332601125\n",
      "  (0, 2171)\t0.16853688094673985\n",
      "  (0, 2155)\t0.23323725332601125\n",
      "  (0, 2148)\t0.23323725332601125\n",
      "  (0, 2147)\t0.2168556884220511\n",
      "  (0, 1876)\t0.2205761976011544\n",
      "  (0, 1875)\t0.20055797891123453\n",
      "  (0, 1420)\t0.23136077869138164\n",
      "  (0, 1418)\t0.18677444227126394\n",
      "  (0, 383)\t0.22790185949049177\n",
      "  (0, 382)\t0.18630876966113138\n",
      "  (0, 321)\t0.23323725332601125\n",
      "  (0, 318)\t0.20860519442993186\n",
      "  (0, 237)\t0.23323725332601125\n",
      "  (0, 236)\t0.21048166906456145\n",
      "  (0, 153)\t0.23323725332601125\n",
      "  (0, 152)\t0.22630010920855226\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tweet: {cleaned_data['text'][5]}\")\n",
    "print('*******************************')\n",
    "print(f\"Vectorised tweet:\\n{x_train[5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed6876a6-b3b9-4f0b-bc6d-26f4697d183e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=150)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import KNN model \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "classifier=KNeighborsClassifier(n_neighbors =150, metric = 'minkowski', p = 2)\n",
    "\n",
    "#train to model\n",
    "\n",
    "classifier.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a78712ea-3f9e-4c0e-bb94-e9611bc88d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc5a7851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[784 102]\n",
      " [289 348]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e1adc5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7432698621142482\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "# Model Accuracy: how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e599c747",
   "metadata": {},
   "source": [
    "# Navie Bayse model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c18b5803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import Navie-Bayse model \n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "classifiy = BernoulliNB()\n",
    "classifiy.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "66cf5eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifiy.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f79e9180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7866053841103086\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "# Model Accuracy: how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5f11d0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[835  51]\n",
      " [274 363]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8e269c",
   "metadata": {},
   "source": [
    "# svm  model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d6081888-d2e2-41ce-a297-31c210a5b98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import svm model\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "\n",
    "#Create a svm Classifier\n",
    "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "\n",
    "\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1e9b9f13-3f02-4136-b868-1a88d3a44f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7872619829284307\n"
     ]
    }
   ],
   "source": [
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "# Model Accuracy: how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f6cddc6c-11a2-4eff-8fb2-5c9bc8f92423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tras(t):\n",
    "    if t == 0:\n",
    "        return \"Not Disaster\"\n",
    "    return \"Disaster\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8eb7c689-be52-4b80-875b-efe6d0396332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>True</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>already know shit go world series armageddon</td>\n",
       "      <td>Not Disaster</td>\n",
       "      <td>Not Disaster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4970</th>\n",
       "      <td>people near meltdown comic free time wait line...</td>\n",
       "      <td>Not Disaster</td>\n",
       "      <td>Not Disaster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>tix calgary flame v col avalanche preseason sc...</td>\n",
       "      <td>Not Disaster</td>\n",
       "      <td>Not Disaster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6362</th>\n",
       "      <td>ever think run choice life rembr kid choice we...</td>\n",
       "      <td>Not Disaster</td>\n",
       "      <td>Not Disaster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>dotish blight car go right ahead mine</td>\n",
       "      <td>Not Disaster</td>\n",
       "      <td>Not Disaster</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet          True  \\\n",
       "311        already know shit go world series armageddon  Not Disaster   \n",
       "4970  people near meltdown comic free time wait line...  Not Disaster   \n",
       "527   tix calgary flame v col avalanche preseason sc...  Not Disaster   \n",
       "6362  ever think run choice life rembr kid choice we...  Not Disaster   \n",
       "800               dotish blight car go right ahead mine  Not Disaster   \n",
       "\n",
       "           predict  \n",
       "311   Not Disaster  \n",
       "4970  Not Disaster  \n",
       "527   Not Disaster  \n",
       "6362  Not Disaster  \n",
       "800   Not Disaster  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = pd.DataFrame({\n",
    "    \"tweet\" : x_test_data,\n",
    "    \"True\" : y_test,\n",
    "    \"predict\" : y_pred \n",
    "})\n",
    "t['True'] = t['True'].apply(tras)\n",
    "t['predict'] = t['predict'].apply(tras)\n",
    "\n",
    "t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018ed371-c471-4ed9-9814-0fae3337bfe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf26a2d6-4314-44de-99a9-e4a00f475daa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470bf822-f84d-4d73-a212-8e0d67522965",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 347.736659,
   "end_time": "2022-05-05T15:14:35.403343",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-05-05T15:08:47.666684",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
